{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Pete\n",
      "[nltk_data]     P\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install torch transformers nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set the device (GPU if available, else CPU)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pete P\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Dialogues:\n",
      "Conversation 1: ['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"]\n",
      "Conversation 2: [\"You're asking me out.  That's so cute. What's your name again?\", 'Forget it.']\n"
     ]
    }
   ],
   "source": [
    "# Load and Parse Data\n",
    "import os\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Initialize the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Set the pad_token to the eos_token and specify padding side\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'  # For decoder-only models like GPT-2\n",
    "\n",
    "# Define the paths to the dataset files\n",
    "lines_file = \"dataset/movie_lines.txt\"\n",
    "conversations_file = \"dataset/movie_conversations.txt\"\n",
    "\n",
    "# Function to load and parse the movie lines\n",
    "def load_lines(file_path):\n",
    "    \"\"\"\n",
    "    Load and parse the movie lines from the dataset.\n",
    "    Returns a dictionary mapping line IDs to text.\n",
    "    \"\"\"\n",
    "    lines = {}\n",
    "    # Open the file with the appropriate encoding to handle special characters\n",
    "    with open(file_path, encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            # Split each line into its components\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            if len(parts) == 5:\n",
    "                line_id = parts[0]  # Unique identifier for the line\n",
    "                text = parts[4]     # The actual dialogue text\n",
    "                lines[line_id] = text  # Store in a dictionary\n",
    "    return lines\n",
    "\n",
    "# Function to load and parse the movie conversations\n",
    "def load_conversations(file_path):\n",
    "    \"\"\"\n",
    "    Load and parse the movie conversations from the dataset.\n",
    "    Returns a list of conversations, each conversation is a list of line IDs.\n",
    "    \"\"\"\n",
    "    conversations = []\n",
    "    # Open the file with the appropriate encoding\n",
    "    with open(file_path, encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            # Split each line into its components\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            if len(parts) == 4:\n",
    "                # The fourth part contains the list of line IDs in a conversation\n",
    "                line_ids = eval(parts[3])  # Convert the string representation of the list to an actual list\n",
    "                conversations.append(line_ids)\n",
    "    return conversations\n",
    "\n",
    "# Function to extract dialogues based on the conversations and lines\n",
    "def extract_dialogues(conversations, lines):\n",
    "    \"\"\"\n",
    "    Extract the dialogues from the conversations using the line IDs.\n",
    "    Returns a list of dialogues, each dialogue is a list of utterances.\n",
    "    \"\"\"\n",
    "    dialogues = []\n",
    "    for conv in conversations:\n",
    "        conv_dialogues = []\n",
    "        for line_id in conv:\n",
    "            if line_id in lines:\n",
    "                conv_dialogues.append(lines[line_id])  # Append the dialogue text for each line ID\n",
    "        dialogues.append(conv_dialogues)\n",
    "    return dialogues\n",
    "\n",
    "# Load the data\n",
    "lines = load_lines(lines_file)\n",
    "conversations = load_conversations(conversations_file)\n",
    "dialogues = extract_dialogues(conversations, lines)\n",
    "\n",
    "# Display sample dialogues for inspection\n",
    "print(\"Sample Dialogues:\")\n",
    "for i in range(2):\n",
    "    print(f\"Conversation {i+1}: {dialogues[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "- The pad_token is set to the eos_token (<|endoftext|>) to handle padding.\n",
    "- The padding side is set to 'left' because GPT-2 is a decoder-only model.\n",
    "- Load the dataset files and parse them to extract dialogues.\n",
    "- The load_lines function reads movie_lines.txt and stores each line's text with its unique ID.\n",
    "- The load_conversations function reads movie_conversations.txt and extracts sequences of line IDs representing conversations.\n",
    "- The extract_dialogues function links line IDs to their actual text to reconstruct the conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Encoded Data:\n",
      "Encoded Sample 1: tensor([ 6090,   356,   787,   428,  2068,    30,   220, 34821, 21952, 14769])... (length: 49)\n",
      "Encoded Sample 2: tensor([ 6090,   356,   787,   428,  2068,    30,   220, 34821, 21952, 14769])... (length: 63)\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum number of previous exchanges to use as context\n",
    "max_history = 5\n",
    "\n",
    "# Function to build input-target pairs from dialogues\n",
    "def build_inputs_targets(dialogues):\n",
    "    \"\"\"\n",
    "    Build input-target pairs for training.\n",
    "    Each input consists of a context (history of utterances),\n",
    "    and the target is the next reply.\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    for conv in dialogues:\n",
    "        for i in range(1, len(conv)):\n",
    "            # Use up to max_history previous exchanges as context\n",
    "            history = conv[max(0, i - max_history):i]\n",
    "            reply = conv[i]  # The current reply\n",
    "            inputs.append((history, reply))  # Store the context and the reply\n",
    "    return inputs\n",
    "\n",
    "# Build the dataset\n",
    "data = build_inputs_targets(dialogues)\n",
    "\n",
    "# Initialize the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# Function to tokenize and encode the data using GPT-2 tokenizer\n",
    "def encode_data(data, tokenizer):\n",
    "    encoded_inputs = []\n",
    "    for history, reply in data:\n",
    "        input_text = \"<|endoftext|>\".join(history) + \"<|endoftext|>\"\n",
    "        full_text = input_text + reply + \"<|endoftext|>\"\n",
    "        encoded = tokenizer.encode(\n",
    "            full_text,\n",
    "            add_special_tokens=False,\n",
    "            max_length=1024,\n",
    "            truncation=True\n",
    "        )\n",
    "        encoded_inputs.append(torch.tensor(encoded))\n",
    "    return encoded_inputs\n",
    "\n",
    "# Encode the data\n",
    "encoded_data = encode_data(data, tokenizer)\n",
    "\n",
    "# Display sample encoded data\n",
    "print(\"\\nSample Encoded Data:\")\n",
    "for i in range(2):\n",
    "    print(f\"Encoded Sample {i+1}: {encoded_data[i][:10]}... (length: {len(encoded_data[i])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare the data by creating input-target pairs, where the input is the conversation history and the target is the reply.\n",
    "- The encode_data function tokenizes and encodes the combined text using the GPT-2 tokenizer.\n",
    "- Ensure that the input sequences are truncated to a maximum length to fit the model's requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 32000\n",
      "Validation set size: 8000\n",
      "Test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom dataset class for our chatbot data\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, encoded_data):\n",
    "        self.data = encoded_data  # Store the encoded data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # Return the total number of samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]  # Retrieve a sample by index\n",
    "\n",
    "# Collate function to pad sequences within a batch and create attention masks and labels\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to process batches of data.\n",
    "    Pads input sequences, creates attention masks and labels.\n",
    "    \"\"\"\n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        batch, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    # Create attention masks (1 where input_ids is not pad_token_id, 0 otherwise)\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    # Create labels; set pad_token_id to -100 so that they are ignored in loss computation\n",
    "    labels = input_ids.clone()\n",
    "    labels[input_ids == tokenizer.pad_token_id] = -100\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "# To use a subset of the data for faster training\n",
    "subset_size = 50000  # Adjust based on our computational resources\n",
    "encoded_data = encoded_data[:subset_size]\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, split the data into training and test sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(encoded_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then, split the training data into training and validation sets (80% train, 20% val)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the sizes of the splits\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = ChatDataset(train_data)\n",
    "val_dataset = ChatDataset(val_data)\n",
    "test_dataset = ChatDataset(test_data)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 6  # Adjust based on available resources\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a custom ChatDataset class to handle our data.\n",
    "- The collate_fn function pads sequences in a batch to the same length.\n",
    "- Use DataLoader to efficiently load data during training.\n",
    "- A subset of the data is used to speed up training when resources are limited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 354,823,168\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Specify the model name; use a smaller model if resources are limited\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Initialize the GPT-2 model\n",
    "model_name = 'gpt2-medium'  # We can use 'gpt2-medium' if using a GPU\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# Display model size\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the pre-trained GPT-2 model and move it to the appropriate device.\n",
    "\n",
    "Load the GPT-2 model (chose 'gpt2-medium') and set up the device (CPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Function to evaluate the model on a validation set\n",
    "def evaluate(model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "    Computes perplexity as the evaluation metric.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            # Forward pass with labels to compute loss\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss  # Mean loss per token in batch\n",
    "            num_tokens = batch['attention_mask'].sum().item()\n",
    "            # Accumulate loss and token counts\n",
    "            total_loss += loss.item() * num_tokens  # Total loss over all tokens in batch\n",
    "            total_tokens += num_tokens\n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The evaluate function computes the perplexity of the model on the validation set.\n",
    "- Perplexity is a measure of how well the model predicts the sample; lower values indicate better performance.\n",
    "- Use torch.no_grad() to disable gradient computation during evaluation for efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|                                                            | 6/5334 [05:55<73:28:03, 49.64s/it]"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Set the number of training epochs\n",
    "epochs = 5  # Increase if computational resources allow\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# Create a learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=500, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Initialize variables for tracking best performance\n",
    "best_val_perplexity = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0\n",
    "    total_train_tokens = 0  # Initialize total_train_tokens\n",
    "    model.train()  # Set the model to training mode\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        # Forward pass with labels to compute loss\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss  # Mean loss per token\n",
    "        num_tokens = batch['attention_mask'].sum().item()\n",
    "        # Backward pass to compute gradients\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        # Accumulate training loss\n",
    "        total_train_loss += loss.item() * num_tokens\n",
    "        total_train_tokens += num_tokens\n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = total_train_loss / total_train_tokens\n",
    "    print(f\"Epoch {epoch+1}, Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    # Evaluate on validation set\n",
    "    val_perplexity = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}, Validation Perplexity: {val_perplexity:.4f}\")\n",
    "    if val_perplexity < best_val_perplexity:\n",
    "        best_val_perplexity = val_perplexity\n",
    "        best_model_state = model.state_dict()\n",
    "        print(f\"Validation perplexity improved; model saved.\")\n",
    "    else:\n",
    "        print(f\"No improvement in validation perplexity.\")\n",
    "\n",
    "# Load the best model state before testing\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "else:\n",
    "    print(\"No improvement during training; using last epoch model.\")\n",
    "\n",
    "# Save the best model\n",
    "model_save_path = 'trained_model_best.pt'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Best model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the model using the AdamW optimizer and a linear learning rate scheduler.\n",
    "- The training loop iterates over epochs and batches, computing loss, performing backpropagation, and updating model parameters.\n",
    "- After each epoch, evaluate the model on the validation set and compute perplexity.\n",
    "- The trained model is saved to a file to avoid retraining in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Pete\n",
      "[nltk_data]     P\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Evaluating:   0%|â–                                                                 | 6/1667 [02:47<12:50:35, 27.84s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data required for BLEU score calculation\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "test_perplexity = evaluate(model, test_loader)\n",
    "print(f\"Final Test Perplexity: {test_perplexity:.4f}\")\n",
    "\n",
    "\n",
    "# Function to calculate the BLEU score on the validation set\n",
    "def calculate_bleu(model, dataloader, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculate the average BLEU score on the validation set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    bleu_scores = []\n",
    "    smooth_fn = SmoothingFunction().method1  # Smoothing function for BLEU score\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Calculating BLEU\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            # Generate predictions\n",
    "            outputs = model.generate(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                max_length=batch['input_ids'].shape[1] + 50,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                do_sample=False,\n",
    "            )\n",
    "            for i in range(batch['input_ids'].size(0)):\n",
    "                # Decode the reference and hypothesis texts\n",
    "                input_len = (batch['input_ids'][i] != tokenizer.pad_token_id).sum().item()\n",
    "                reference_ids = batch['input_ids'][i][input_len:].tolist()\n",
    "                hypothesis_ids = outputs[i][input_len:].tolist()\n",
    "                reference = tokenizer.decode(reference_ids, skip_special_tokens=True)\n",
    "                hypothesis = tokenizer.decode(hypothesis_ids, skip_special_tokens=True)\n",
    "                # Tokenize the texts\n",
    "                reference_tokens = nltk.word_tokenize(reference)\n",
    "                hypothesis_tokens = nltk.word_tokenize(hypothesis)\n",
    "                # Compute BLEU score\n",
    "                bleu = sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smooth_fn)\n",
    "                bleu_scores.append(bleu)\n",
    "    # Calculate average BLEU score\n",
    "    average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    return average_bleu\n",
    "\n",
    "# Calculate BLEU score on the validation set\n",
    "bleu_score = calculate_bleu(model, val_loader, tokenizer)\n",
    "print(f\"Validation BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The calculate_bleu function evaluates the model's predictions against the ground truth using the BLEU score.\n",
    "- BLEU score measures the similarity between the generated text and reference text.\n",
    "- Smoothing is applied to handle cases where the hypothesis and reference have few overlapping n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction with the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Content Filtering\n",
    "import re\n",
    "\n",
    "# List of disallowed words or phrases to filter out\n",
    "disallowed_words = [\n",
    "    # Offensive Language\n",
    "    r'self_harm', r'suicide', r'death', r'kill',\n",
    "    # Profanity (replace with actual words or leave as *)\n",
    "    #r'\\b(f**k|s**t|b***h|d**n|a**hole|c**t|sl*t|pr*ck|b**tard)\\b',\n",
    "    # Slurs or Hate Speech \n",
    "    #r'\\b(racial_slur1|racial_slur2|homophobic_slur1|homophobic_slur2|gender_slur|ethnic_slur|religious_slur)\\b',  \n",
    "    # Explicit Sexual Content\n",
    "    r'\\b(porn|nude|sex|explicit|xxx|erotic|fetish|incest|rape|molest)\\b',\n",
    "    # Spam or Malicious Content\n",
    "    #r'\\b(spam|phishing|malware|virus|adware|trojan|scam|fraud)\\b',\n",
    "    # Violence or Graphic Content\n",
    "    r'\\b(assault|gun|shooting|bomb|explosion|attack|blood|mutilation|massacre)\\b',\n",
    "    # Misinformation and Conspiracy Theories\n",
    "    #r'\\b(fake_news|hoax|conspiracy|anti_vax|flat_earth|election_fraud)\\b',\n",
    "    # Harmful Behavior or Self-Harm\n",
    "    r'\\b(cutting|eating_disorder|anorexia|bulimia|starving|addiction)\\b',\n",
    "]\n",
    "\n",
    "# Escape special regex characters and compile the pattern\n",
    "pattern = re.compile(r'\\b(' + '|'.join([re.escape(word) for word in disallowed_words]) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "def contains_disallowed_content(text):\n",
    "    return bool(pattern.search(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define patterns for disallowed content to prevent the chatbot from generating inappropriate responses.\n",
    "- The contains_disallowed_content function checks if the generated text contains any disallowed words or phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self, model, tokenizer, max_history=4):\n",
    "        self.model = model  # The language generation model\n",
    "        self.tokenizer = tokenizer  # Tokenizer to encode/decode text\n",
    "        self.max_history = max_history  # Number of previous exchanges to consider\n",
    "        self.chat_history = []  # Stores the conversation history\n",
    "        self.device = device  # Ensure device is set\n",
    "        # Set pad_token to eos_token for models like GPT-2 that do not have a pad_token\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def reset_history(self):\n",
    "        self.chat_history = []\n",
    "\n",
    "    def get_response(self, user_input):\n",
    "        # Detect inappropriate or harmful content\n",
    "        if contains_disallowed_content(user_input):\n",
    "            return \"I'm sorry, but I cannot assist with that request.\"\n",
    "            \n",
    "        self.chat_history.append(user_input)\n",
    "        # Keep only the last max_history exchanges\n",
    "        history = self.chat_history[-self.max_history:]\n",
    "        # Combine the history into a single input string\n",
    "        input_text = \"<|endoftext|>\".join(history) + \"<|endoftext|>\"\n",
    "        \n",
    "        # Encode the input text with truncation and padding\n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors='pt',\n",
    "            max_length=512,  # Truncate if necessary\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        # Generate a response\n",
    "        output = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=50,  # Generate up to 50 new tokens\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            do_sample=True,  # Use sampling for more varied responses\n",
    "            top_p=0.92,      # Nucleus sampling parameter\n",
    "            top_k=40,        # Top-k sampling parameter\n",
    "            temperature=0.8, # Sampling temperature\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            num_return_sequences=1,  # Generate one response at a time\n",
    "            repetition_penalty=1.3,  # Penalize repetition for better response diversity\n",
    "            no_repeat_ngram_size=3   # Prevent repetition of n-grams\n",
    "        )\n",
    "        # Extract the generated tokens beyond the input length\n",
    "        generated_tokens = output[0][input_ids.size(-1):]\n",
    "        # Decode the generated tokens to text\n",
    "        reply = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        # Apply content filtering\n",
    "        if contains_disallowed_content(reply):\n",
    "            reply = \"I'm sorry, but I can't assist with that request.\"\n",
    "            \n",
    "        self.chat_history.append(reply)\n",
    "        return reply.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Chatbot class encapsulates the chatbot's functionality.\n",
    "- It maintains a conversation history to provide context for multi-turn conversations.\n",
    "- The get_response method generates a response using the model and applies content filtering.\n",
    "- Sampling parameters (top_p, top_k, temperature) control the diversity of the generated responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pete P\\AppData\\Local\\Temp\\ipykernel_21932\\1695062528.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_load_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Specify the model name used during training\n",
    "model_name = 'gpt2-medium'  # Replace with 'gpt2', 'gpt2-large', etc., as appropriate\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# Load the model architecture\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Load the saved model weights\n",
    "model_load_path = 'trained_model.pt'\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
    "    print(\"Model loaded successfully!\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error loading model state_dict: {e}\")\n",
    "    print(\"Please ensure that the model architecture matches the saved model weights.\")\n",
    "    exit(1)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the saved model weights into the model architecture.\n",
    "- The model is set to evaluation mode to disable dropout and other training-specific layers.\n",
    "- This allows us to use the trained model for generating responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the Chatbot via Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hello! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Who are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: My name is Marcus. I am a professional thief and carjacker who likes to gamble. I'm also an ex-cop, but I still like the idea of a good story for my clients and myself in this area... So if we got\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Are you good with your work? Sounds not fun haha\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: And what's better than good news at that moment? News from a young woman whose life has been turned upside down when she finds out her mother was killed by another man named Frank.  She wants revenge! What does she want? Is it love\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Do take care of your self\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: You will never find me without a watch. I was given one as part 'a kindness from a friend'.  Now listen carefully: \"A dear old lady gave us a crystal timepiece.\" That's all there would be. All those little words\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I had the same problem with my original order, but they sent an e-mail to let you know we were back on track and they wanted something in return. They offered up ten thousand for whatever this is -- which sounds like a lot to you\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  goodbye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Goodbye! Have a great day.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the chatbot\n",
    "chatbot = Chatbot(model, tokenizer)\n",
    "\n",
    "# Start the conversation loop\n",
    "print(\"Bot: Hello! How can I assist you today?\")\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"goodbye\"]:\n",
    "        print(\"Bot: Goodbye! Have a great day.\")\n",
    "        break\n",
    "    response = chatbot.get_response(user_input)\n",
    "    print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an instance of the Chatbot class.\n",
    "- A loop allows the user to interact with the chatbot via the console.\n",
    "- The conversation continues until the user types an exit command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
