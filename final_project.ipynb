{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Display movie_lines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
      "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
      "L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\n",
      "L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"dataset/movie_lines.txt\"\n",
    "\n",
    "# Load and display the first few lines\n",
    "def load_and_inspect_data(file_path, num_lines=5):\n",
    "    with open(file_path, encoding='iso-8859-1') as f:  # Using 'iso-8859-1' encoding to handle special characters\n",
    "        for i in range(num_lines):\n",
    "            print(f.readline().strip())\n",
    "\n",
    "# Load and display the first 5 lines of the movie_lines.txt file\n",
    "load_and_inspect_data(dataset_path, num_lines=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the above, we see each line is structured in the following format:\n",
    "\n",
    "- L1045: Line ID\n",
    "- u0: User ID (who is speaking)\n",
    "- m0: Movie ID (which movie the line is from)\n",
    "- BIANCA: Character name\n",
    "- They do not!: The actual line spoken\n",
    "\n",
    "We'll focus on extracting the dialogue (the last part) for building the chatbot, because the chatbot's responses are based on actual lines spoken in conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and Store Movie Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines parsed: 304446\n",
      "L1045: They do not!\n",
      "L1044: They do to!\n",
      "L985: I hope so.\n",
      "L984: She okay?\n",
      "L925: Let's go.\n"
     ]
    }
   ],
   "source": [
    "# Parse the movie_lines.txt file and store it in a dictionary\n",
    "def parse_movie_lines(file_path):\n",
    "    lines_dict = {}\n",
    "    with open(file_path, encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            if len(parts) == 5:\n",
    "                line_id = parts[0]  # Line ID\n",
    "                dialogue = parts[4]  # Actual line\n",
    "                lines_dict[line_id] = dialogue\n",
    "    return lines_dict\n",
    "\n",
    "# Parse the movie_lines.txt file\n",
    "movie_lines_dict = parse_movie_lines(dataset_path)\n",
    "\n",
    "# Check the size of the dictionary and print some examples\n",
    "print(f\"Total lines parsed: {len(movie_lines_dict)}\")\n",
    "for i, (line_id, dialogue) in enumerate(movie_lines_dict.items()):\n",
    "    if i < 5:  # Print the first 5 parsed lines\n",
    "        print(f\"{line_id}: {dialogue}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and Link Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total conversations parsed: 83097\n",
      "Conversation 1: ['L194', 'L195', 'L196', 'L197']\n",
      "Conversation 2: ['L198', 'L199']\n",
      "Conversation 3: ['L200', 'L201', 'L202', 'L203']\n",
      "Conversation 4: ['L204', 'L205', 'L206']\n",
      "Conversation 5: ['L207', 'L208']\n"
     ]
    }
   ],
   "source": [
    "# Define the path for the movie_conversations.txt file\n",
    "conversations_path = \"dataset/movie_conversations.txt\"\n",
    "\n",
    "# Parse the movie_conversations.txt file and store conversations as lists of line IDs\n",
    "def parse_movie_conversations(file_path):\n",
    "    conversations = []\n",
    "    with open(file_path, encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            if len(parts) == 4:\n",
    "                # The last part contains the list of line IDs as a string, e.g., \"['L1045', 'L1044', ...]\"\n",
    "                line_ids_str = parts[3]\n",
    "                # Convert the string representation of the list into an actual Python list\n",
    "                line_ids = eval(line_ids_str)  # This will transform the string into a list\n",
    "                conversations.append(line_ids)\n",
    "    return conversations\n",
    "\n",
    "# Parse the movie_conversations.txt file\n",
    "movie_conversations = parse_movie_conversations(conversations_path)\n",
    "\n",
    "# Check the number of conversations and print a few examples\n",
    "print(f\"Total conversations parsed: {len(movie_conversations)}\")\n",
    "for i, conversation in enumerate(movie_conversations):\n",
    "    if i < 5:  # Print the first 5 parsed conversations (line IDs only)\n",
    "        print(f\"Conversation {i+1}: {conversation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Line IDs to Dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation 1: ['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"]\n",
      "Conversation 2: [\"You're asking me out.  That's so cute. What's your name again?\", 'Forget it.']\n",
      "Conversation 3: [\"No, no, it's my fault -- we didn't have a proper introduction ---\", 'Cameron.', \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\", 'Seems like she could get a date easy enough...']\n"
     ]
    }
   ],
   "source": [
    "# Link the line IDs from conversations to the actual dialogues from movie_lines_dict\n",
    "def link_conversations_to_dialogues(conversations, lines_dict):\n",
    "    full_conversations = []\n",
    "    for conversation in conversations:\n",
    "        conv_dialogues = []\n",
    "        for line_id in conversation:\n",
    "            if line_id in lines_dict:\n",
    "                conv_dialogues.append(lines_dict[line_id])\n",
    "        full_conversations.append(conv_dialogues)\n",
    "    return full_conversations\n",
    "\n",
    "# Link conversations to the actual dialogues\n",
    "linked_conversations = link_conversations_to_dialogues(movie_conversations, movie_lines_dict)\n",
    "\n",
    "# Check the first few complete conversations\n",
    "for i, conversation in enumerate(linked_conversations):\n",
    "    if i < 3:  # Print the first 3 full conversations (actual dialogues)\n",
    "        print(f\"Conversation {i+1}: {conversation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output: Linked the line IDs to their corresponding actual dialogues, creating complete conversations, so that the chatbot can learn how to handle multi-turn interactions and contextual responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Conversation 1: [['can', 'we', 'make', 'this', 'quick', '?', 'roxanne', 'korrine', 'and', 'andrew', 'barrett', 'are', 'having', 'an', 'incredibly', 'horrendous', 'public', 'break-', 'up', 'on', 'the', 'quad', '.', 'again', '.'], ['well', ',', 'i', 'thought', 'we', \"'d\", 'start', 'with', 'pronunciation', ',', 'if', 'that', \"'s\", 'okay', 'with', 'you', '.'], ['not', 'the', 'hacking', 'and', 'gagging', 'and', 'spitting', 'part', '.', 'please', '.'], ['okay', '...', 'then', 'how', \"'bout\", 'we', 'try', 'out', 'some', 'french', 'cuisine', '.', 'saturday', '?', 'night', '?']]\n",
      "Tokenized Conversation 2: [['you', \"'re\", 'asking', 'me', 'out', '.', 'that', \"'s\", 'so', 'cute', '.', 'what', \"'s\", 'your', 'name', 'again', '?'], ['forget', 'it', '.']]\n",
      "Tokenized Conversation 3: [['no', ',', 'no', ',', 'it', \"'s\", 'my', 'fault', '--', 'we', 'did', \"n't\", 'have', 'a', 'proper', 'introduction', '--', '-'], ['cameron', '.'], ['the', 'thing', 'is', ',', 'cameron', '--', 'i', \"'m\", 'at', 'the', 'mercy', 'of', 'a', 'particularly', 'hideous', 'breed', 'of', 'loser', '.', 'my', 'sister', '.', 'i', 'ca', \"n't\", 'date', 'until', 'she', 'does', '.'], ['seems', 'like', 'she', 'could', 'get', 'a', 'date', 'easy', 'enough', '...']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize each conversation\n",
    "def tokenize_conversations(conversations):\n",
    "    tokenized_conversations = []\n",
    "    for conversation in conversations:\n",
    "        tokenized_conversation = [word_tokenize(sentence.lower()) for sentence in conversation]  # Tokenize each sentence and convert to lowercase\n",
    "        tokenized_conversations.append(tokenized_conversation)\n",
    "    return tokenized_conversations\n",
    "\n",
    "# Tokenize the conversations\n",
    "tokenized_conversations = tokenize_conversations(linked_conversations)\n",
    "\n",
    "# Check a few tokenized conversations\n",
    "for i, conversation in enumerate(tokenized_conversations):\n",
    "    if i < 3:  # Print first 3 tokenized conversations\n",
    "        print(f\"Tokenized Conversation {i+1}: {conversation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary and Convert Tokens to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation 1 (as IDs): [[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 24], [26, 27, 28, 29, 3, 30, 31, 32, 33, 27, 34, 35, 36, 37, 32, 38, 24], [39, 22, 40, 10, 41, 10, 42, 43, 24, 44, 24], [37, 45, 46, 47, 48, 3, 49, 50, 51, 52, 53, 24, 54, 7, 55, 7]]\n",
      "Conversation 2 (as IDs): [[38, 56, 57, 58, 50, 24, 35, 36, 59, 60, 24, 61, 36, 62, 63, 25, 7], [64, 65, 24]]\n",
      "Conversation 3 (as IDs): [[66, 27, 66, 27, 65, 36, 67, 68, 69, 3, 70, 71, 72, 73, 74, 75, 69, 76], [77, 24], [22, 78, 79, 27, 77, 69, 28, 80, 81, 22, 82, 83, 73, 84, 85, 86, 83, 87, 24, 67, 88, 24, 28, 89, 71, 90, 91, 92, 93, 24], [94, 95, 92, 96, 97, 73, 90, 98, 99, 45]]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Build a vocabulary dictionary mapping each word to a unique index\n",
    "def build_vocabulary(tokenized_conversations):\n",
    "    vocab = defaultdict(lambda: len(vocab))  # Assigns an incrementing ID to each new word\n",
    "    vocab['<PAD>'] = 0  # Reserve 0 for padding\n",
    "    vocab['<UNK>'] = 1  # Reserve 1 for unknown words\n",
    "\n",
    "    for conversation in tokenized_conversations:\n",
    "        for sentence in conversation:\n",
    "            for word in sentence:\n",
    "                vocab[word]  # Adds word to vocab if not already present\n",
    "\n",
    "    return dict(vocab)\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = build_vocabulary(tokenized_conversations)\n",
    "\n",
    "# Convert tokenized conversations to sequences of word IDs\n",
    "def convert_to_ids(tokenized_conversations, vocab):\n",
    "    conversations_ids = []\n",
    "    for conversation in tokenized_conversations:\n",
    "        conv_ids = [[vocab.get(word, vocab['<UNK>']) for word in sentence] for sentence in conversation]\n",
    "        conversations_ids.append(conv_ids)\n",
    "    return conversations_ids\n",
    "\n",
    "# Convert tokenized conversations to IDs\n",
    "conversations_ids = convert_to_ids(tokenized_conversations, vocab)\n",
    "\n",
    "# Check the first few conversations as IDs\n",
    "for i, conversation in enumerate(conversations_ids):\n",
    "    if i < 3:  # Print first 3 conversations\n",
    "        print(f\"Conversation {i+1} (as IDs): {conversation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the conversations now are converted into sequences of numerical IDs. Each word in the conversation is mapped to a an ID based on the vocabulary we built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Conversation 1: [[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [26, 27, 28, 29, 3, 30, 31, 32, 33, 27, 34, 35, 36, 37, 32, 38, 24, 0, 0, 0], [39, 22, 40, 10, 41, 10, 42, 43, 24, 44, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0], [37, 45, 46, 47, 48, 3, 49, 50, 51, 52, 53, 24, 54, 7, 55, 7, 0, 0, 0, 0]]\n",
      "Padded Conversation 2: [[38, 56, 57, 58, 50, 24, 35, 36, 59, 60, 24, 61, 36, 62, 63, 25, 7, 0, 0, 0], [64, 65, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "Padded Conversation 3: [[66, 27, 66, 27, 65, 36, 67, 68, 69, 3, 70, 71, 72, 73, 74, 75, 69, 76, 0, 0], [77, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [22, 78, 79, 27, 77, 69, 28, 80, 81, 22, 82, 83, 73, 84, 85, 86, 83, 87, 24, 67], [94, 95, 92, 96, 97, 73, 90, 98, 99, 45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences so that all sentences in a conversation have the same length\n",
    "def pad_conversations(conversations_ids, max_length=20):\n",
    "    padded_conversations = []\n",
    "    for conversation in conversations_ids:\n",
    "        padded_conversation = []\n",
    "        for sentence in conversation:\n",
    "            # If sentence length is less than max_length, pad with 0s, else truncate\n",
    "            padded_sentence = sentence[:max_length] + [0] * (max_length - len(sentence))\n",
    "            padded_conversation.append(padded_sentence)\n",
    "        padded_conversations.append(padded_conversation)\n",
    "    return padded_conversations\n",
    "\n",
    "# Pad the conversations\n",
    "padded_conversations = pad_conversations(conversations_ids, max_length=20)\n",
    "\n",
    "# Check the first few padded conversations\n",
    "for i, conversation in enumerate(padded_conversations):\n",
    "    if i < 3:  # Print first 3 padded conversations\n",
    "        print(f\"Padded Conversation {i+1}: {conversation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all sentences have a uniform length for easy processing for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 243556\n",
      "Test set size: 60890\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Flatten conversations so they can be split into training and test sets\n",
    "def flatten_conversations(padded_conversations):\n",
    "    all_conversations = []\n",
    "    for conversation in padded_conversations:\n",
    "        all_conversations.extend(conversation)  # Add each sentence to the list\n",
    "    return all_conversations\n",
    "\n",
    "# Flatten the conversations\n",
    "all_sentences = flatten_conversations(padded_conversations)\n",
    "\n",
    "# Split the data into 80% training and 20% test\n",
    "train_sentences, test_sentences = train_test_split(all_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the sizes of the train and test sets\n",
    "print(f\"Training set size: {len(train_sentences)}\")\n",
    "print(f\"Test set size: {len(test_sentences)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completed as of September 26th:\n",
    "- Loaded and parsed the `movie_lines.txt` and `movie_conversations.txt`.\n",
    "- Linked dialogue lines to create full conversations.\n",
    "- Tokenized and padded the conversations.\n",
    "- Built a vocabulary and converted the conversations to numerical IDs.\n",
    "- Split the data into training and test sets.\n",
    "\n",
    "### Next Steps:\n",
    "- Model Design and Training with model architecture.\n",
    "- Train the model using the processed training data.\n",
    "- Evaluate the model on the test data.\n",
    "- Tune the model (if needed).\n",
    "- Convert the project to a chatbot accessible via a webpage using Flask and Bootstrap.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
