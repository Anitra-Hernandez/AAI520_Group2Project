{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch transformers nltk\n",
    "\n",
    "# import nessaries libraries:\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Set the device (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Dialogues:\n",
      "Conversation 1: ['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"]\n",
      "Conversation 2: [\"You're asking me out.  That's so cute. What's your name again?\", 'Forget it.']\n"
     ]
    }
   ],
   "source": [
    "# Load and Parse Data\n",
    "\n",
    "# Initialize the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "\n",
    "# Set the pad_token to the eos_token and specify padding side\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'  # For decoder-only models like GPT-2\n",
    "\n",
    "# Define the paths to the dataset files\n",
    "lines_file = \"dataset/movie_lines.txt\"\n",
    "conversations_file = \"dataset/movie_conversations.txt\"\n",
    "\n",
    "# Function to load and parse the movie lines\n",
    "def load_lines(file_path):\n",
    "    \"\"\"\n",
    "    Load and parse the movie lines from the dataset.\n",
    "    Returns a dictionary mapping line IDs to text.\n",
    "    \"\"\"\n",
    "    lines = {}\n",
    "    # Open the file with the appropriate encoding to handle special characters\n",
    "    with open(file_path, encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            # Split each line into its components\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            if len(parts) == 5:\n",
    "                line_id = parts[0]  # Unique identifier for the line\n",
    "                text = parts[4]     # The actual dialogue text\n",
    "                lines[line_id] = text  # Store in a dictionary\n",
    "    return lines\n",
    "\n",
    "# Function to load and parse the movie conversations\n",
    "def load_conversations(file_path):\n",
    "    \"\"\"\n",
    "    Load and parse the movie conversations from the dataset.\n",
    "    Returns a list of conversations, each conversation is a list of line IDs.\n",
    "    \"\"\"\n",
    "    conversations = []\n",
    "    # Open the file with the appropriate encoding\n",
    "    with open(file_path, encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            # Split each line into its components\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            if len(parts) == 4:\n",
    "                # The fourth part contains the list of line IDs in a conversation\n",
    "                line_ids = eval(parts[3])  # Convert the string representation of the list to an actual list\n",
    "                conversations.append(line_ids)\n",
    "    return conversations\n",
    "\n",
    "# Function to extract dialogues based on the conversations and lines\n",
    "def extract_dialogues(conversations, lines):\n",
    "    \"\"\"\n",
    "    Extract the dialogues from the conversations using the line IDs.\n",
    "    Returns a list of dialogues, each dialogue is a list of utterances.\n",
    "    \"\"\"\n",
    "    dialogues = []\n",
    "    for conv in conversations:\n",
    "        conv_dialogues = []\n",
    "        for line_id in conv:\n",
    "            if line_id in lines:\n",
    "                conv_dialogues.append(lines[line_id])  # Append the dialogue text for each line ID\n",
    "        dialogues.append(conv_dialogues)\n",
    "    return dialogues\n",
    "\n",
    "# Load the data\n",
    "lines = load_lines(lines_file)\n",
    "conversations = load_conversations(conversations_file)\n",
    "dialogues = extract_dialogues(conversations, lines)\n",
    "\n",
    "# Display sample dialogues for inspection\n",
    "print(\"Sample Dialogues:\")\n",
    "for i in range(2):\n",
    "    print(f\"Conversation {i+1}: {dialogues[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "- The pad_token is set to the eos_token (<|endoftext|>) to handle padding.\n",
    "- The padding side is set to 'left' because GPT-2 is a decoder-only model.\n",
    "- Load the dataset files and parse them to extract dialogues.\n",
    "- The load_lines function reads movie_lines.txt and stores each line's text with its unique ID.\n",
    "- The load_conversations function reads movie_conversations.txt and extracts sequences of line IDs representing conversations.\n",
    "- The extract_dialogues function links line IDs to their actual text to reconstruct the conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Sample Encoded Data:\n",
      "Encoded Sample 1: tensor([ 6090,   356,   787,   428,  2068,    30,   220, 34821, 21952, 14769],\n",
      "       device='cuda:0')... (length: 49)\n",
      "Encoded Sample 2: tensor([ 6090,   356,   787,   428,  2068,    30,   220, 34821, 21952, 14769],\n",
      "       device='cuda:0')... (length: 63)\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum number of previous exchanges to use as context\n",
    "max_history = 5\n",
    "\n",
    "# Function to build input-target pairs from dialogues\n",
    "def build_inputs_targets(dialogues):\n",
    "    \"\"\"\n",
    "    Build input-target pairs for training.\n",
    "    Each input consists of a context (history of utterances),\n",
    "    and the target is the next reply.\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    for conv in dialogues:\n",
    "        for i in range(1, len(conv)):\n",
    "            # Use up to max_history previous exchanges as context\n",
    "            history = conv[max(0, i - max_history):i]\n",
    "            reply = conv[i]  # The current reply\n",
    "            inputs.append((history, reply))  # Store the context and the reply\n",
    "    return inputs\n",
    "\n",
    "# Build the dataset\n",
    "data = build_inputs_targets(dialogues)\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to tokenize and encode the data using GPT-2 tokenizer\n",
    "def encode_data(data, tokenizer):\n",
    "    encoded_inputs = []\n",
    "    for history, reply in data:\n",
    "        input_text = \"<|endoftext|>\".join(history) + \"<|endoftext|>\"\n",
    "        full_text = input_text + reply + \"<|endoftext|>\"\n",
    "        encoded = tokenizer.encode(\n",
    "            full_text,\n",
    "            add_special_tokens=False,\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        )\n",
    "        # Move encoded data to GPU if available\n",
    "        encoded_inputs.append(torch.tensor(encoded).to(device))\n",
    "    return encoded_inputs\n",
    "\n",
    "# Encode the data and move to device\n",
    "encoded_data = encode_data(data, tokenizer)\n",
    "\n",
    "# Display sample encoded data\n",
    "print(\"\\nSample Encoded Data:\")\n",
    "for i in range(2):\n",
    "    print(f\"Encoded Sample {i+1}: {encoded_data[i][:10]}... (length: {len(encoded_data[i])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare the data by creating input-target pairs, where the input is the conversation history and the target is the reply.\n",
    "- The encode_data function tokenizes and encodes the combined text using the GPT-2 tokenizer.\n",
    "- Ensure that the input sequences are truncated to a maximum length to fit the model's requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training set size: 19200\n",
      "Validation set size: 4800\n",
      "Test set size: 6000\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Custom dataset class for our chatbot data\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, encoded_data):\n",
    "        self.data = encoded_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Collate function to pad sequences within a batch and create attention masks and labels\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        batch, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    ).to(device)  # Move to device (GPU or CPU)\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long().to(device)  # Move to device\n",
    "    labels = input_ids.clone().to(device)  # Move to device\n",
    "    labels[input_ids == tokenizer.pad_token_id] = -100\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "# To use a smaller subset of the data for faster training\n",
    "subset_size = 30000  # Adjust based on our computational resources\n",
    "encoded_data = encoded_data[:subset_size]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, split the data into training and test sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(encoded_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then, split the training data into training and validation sets (80% train, 20% val)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the sizes of the splits\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = ChatDataset(train_data)\n",
    "val_dataset = ChatDataset(val_data)\n",
    "test_dataset = ChatDataset(test_data)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 4  # Adjust based on available resources\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a custom ChatDataset class to handle our data.\n",
    "- The collate_fn function pads sequences in a batch to the same length.\n",
    "- Use DataLoader to efficiently load data during training.\n",
    "- A subset of the data is used to speed up training when resources are limited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 354,823,168\n"
     ]
    }
   ],
   "source": [
    "# Specify the model name; use a smaller model if resources are limited\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Initialize the GPT-2 model\n",
    "model_name = 'gpt2-medium'  # We can use 'gpt2-medium' if using a GPU\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# Display model size\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the pre-trained GPT-2 model and move it to the appropriate device.\n",
    "\n",
    "Load the GPT-2 model (chose 'gpt2-medium') and set up the device (CPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation Metrics and Implement Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning the Model\n",
    "epochs = 3  # Number of training epochs\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "# Function to evaluate the model on a validation set\n",
    "def evaluate(model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "    Computes perplexity as the evaluation metric.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            # Forward pass with labels to compute loss\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss  # Mean loss per token in batch\n",
    "            num_tokens = batch['attention_mask'].sum().item()\n",
    "            # Accumulate loss and token counts\n",
    "            total_loss += loss.item() * num_tokens  # Total loss over all tokens in batch\n",
    "            total_tokens += num_tokens\n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "# Function to calculate the BLEU score on the validation set\n",
    "def calculate_bleu(model, dataloader, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculate the average BLEU score on the validation set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    bleu_scores = []\n",
    "    smooth_fn = SmoothingFunction().method1  # Smoothing function for BLEU score\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Calculating BLEU\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            # Generate predictions\n",
    "            outputs = model.generate(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                max_length=batch['input_ids'].shape[1] + 50,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                do_sample=False,\n",
    "            )\n",
    "            for i in range(batch['input_ids'].size(0)):\n",
    "                # Decode the reference and hypothesis texts\n",
    "                input_len = (batch['input_ids'][i] != tokenizer.pad_token_id).sum().item()\n",
    "                reference_ids = batch['input_ids'][i][input_len:].tolist()\n",
    "                hypothesis_ids = outputs[i][input_len:].tolist()\n",
    "                reference = tokenizer.decode(reference_ids, skip_special_tokens=True)\n",
    "                hypothesis = tokenizer.decode(hypothesis_ids, skip_special_tokens=True)\n",
    "                # Tokenize the texts\n",
    "                reference_tokens = nltk.word_tokenize(reference)\n",
    "                hypothesis_tokens = nltk.word_tokenize(hypothesis)\n",
    "                # Compute BLEU score\n",
    "                bleu = sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smooth_fn)\n",
    "                bleu_scores.append(bleu)\n",
    "    # Calculate average BLEU score\n",
    "    average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    return average_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The evaluate function computes the perplexity of the model on the validation set.\n",
    "- Perplexity is a measure of how well the model predicts the sample; lower values indicate better performance.\n",
    "- Use torch.no_grad() to disable gradient computation during evaluation for efficiency.\n",
    "\n",
    "- The calculate_bleu function evaluates the model's predictions against the ground truth using the BLEU score.\n",
    "- BLEU score measures the similarity between the generated text and reference text.\n",
    "- Smoothing is applied to handle cases where the hypothesis and reference have few overlapping n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 4800/4800 [13:45<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Training Loss: 3.1258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1200/1200 [00:50<00:00, 23.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Perplexity: 16.1776\n",
      "Validation perplexity improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 4800/4800 [13:44<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Training Loss: 2.5341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1200/1200 [00:51<00:00, 23.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Validation Perplexity: 12.4245\n",
      "Validation perplexity improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 4800/4800 [13:57<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Training Loss: 2.1949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1200/1200 [00:51<00:00, 23.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Validation Perplexity: 11.4023\n",
      "Validation perplexity improved; model saved.\n",
      "Best model saved to fine_tuned_gpt2_medium.pt\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables for tracking best performance\n",
    "best_val_perplexity = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = 0\n",
    "    total_train_tokens = 0  # Initialize total_train_tokens\n",
    "    model.train()  # Set the model to training mode\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        # Forward pass with labels to compute loss\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss  # Mean loss per token\n",
    "        num_tokens = batch['attention_mask'].sum().item()\n",
    "        # Backward pass to compute gradients\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        # Accumulate training loss\n",
    "        total_train_loss += loss.item() * num_tokens\n",
    "        total_train_tokens += num_tokens\n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = total_train_loss / total_train_tokens\n",
    "    print(f\"Epoch {epoch+1}, Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    # Evaluate on validation set\n",
    "    val_perplexity = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}, Validation Perplexity: {val_perplexity:.4f}\")\n",
    "    if val_perplexity < best_val_perplexity:\n",
    "        best_val_perplexity = val_perplexity\n",
    "        best_model_state = model.state_dict()\n",
    "        print(f\"Validation perplexity improved; model saved.\")\n",
    "    else:\n",
    "        print(f\"No improvement in validation perplexity.\")\n",
    "\n",
    "# Load the best model state before testing\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "else:\n",
    "    print(\"No improvement during training; using last epoch model.\")\n",
    "\n",
    "# Save the best model\n",
    "model_save_path = 'fine_tuned_gpt2_medium.pt'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Best model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the model using the AdamW optimizer and a linear learning rate scheduler.\n",
    "- The training loop iterates over epochs and batches, computing loss, performing backpropagation, and updating model parameters.\n",
    "- After each epoch, evaluate the model on the validation set and compute perplexity.\n",
    "- The trained model is saved to a file to avoid retraining in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1500/1500 [01:03<00:00, 23.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Perplexity: 11.5412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BLEU: 100%|██████████| 1200/1200 [28:03<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU Score: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the trained model\n",
    "model_save_path = 'fine_tuned_gpt2_medium.pt'  # Define the model path here\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "    print(\"Model loaded successfully!\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error loading model state_dict: {e}\")\n",
    "    print(\"Please ensure that the model architecture matches the saved model weights.\")\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "test_perplexity = evaluate(model, test_loader)\n",
    "print(f\"Final Test Perplexity: {test_perplexity:.4f}\")\n",
    "\n",
    "# Calculate BLEU score on the validation set\n",
    "bleu_score = calculate_bleu(model, val_loader, tokenizer)\n",
    "print(f\"Validation BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the saved model weights into the model architecture.\n",
    "- The model is set to evaluation mode to disable dropout and other training-specific layers.\n",
    "- This allows us to use the trained model for generating responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction with the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Content Filtering\n",
    "import re\n",
    "\n",
    "# List of disallowed words or phrases to filter out\n",
    "disallowed_words = [\n",
    "    # Offensive Language\n",
    "    r'self_harm', r'suicide', r'death', r'kill',\n",
    "    # Profanity (replace with actual words or leave as *)\n",
    "    #r'\\b(f**k|s**t|b***h|d**n|a**hole|c**t|sl*t|pr*ck|b**tard)\\b',\n",
    "    # Slurs or Hate Speech \n",
    "    #r'\\b(racial_slur1|racial_slur2|homophobic_slur1|homophobic_slur2|gender_slur|ethnic_slur|religious_slur)\\b',  \n",
    "    # Explicit Sexual Content\n",
    "    r'\\b(porn|nude|sex|explicit|xxx|erotic|fetish|incest|rape|molest)\\b',\n",
    "    # Spam or Malicious Content\n",
    "    #r'\\b(spam|phishing|malware|virus|adware|trojan|scam|fraud)\\b',\n",
    "    # Violence or Graphic Content\n",
    "    r'\\b(assault|gun|shooting|bomb|explosion|attack|blood|mutilation|massacre)\\b',\n",
    "    # Misinformation and Conspiracy Theories\n",
    "    #r'\\b(fake_news|hoax|conspiracy|anti_vax|flat_earth|election_fraud)\\b',\n",
    "    # Harmful Behavior or Self-Harm\n",
    "    r'\\b(cutting|eating_disorder|anorexia|bulimia|starving|addiction)\\b',\n",
    "]\n",
    "\n",
    "# Escape special regex characters and compile the pattern\n",
    "pattern = re.compile(r'\\b(' + '|'.join([re.escape(word) for word in disallowed_words]) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "def contains_disallowed_content(text):\n",
    "    return bool(pattern.search(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define patterns for disallowed content to prevent the chatbot from generating inappropriate responses.\n",
    "- The contains_disallowed_content function checks if the generated text contains any disallowed words or phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self, model, tokenizer, max_history=4):\n",
    "        self.model = model  # The language generation model\n",
    "        self.tokenizer = tokenizer  # Tokenizer to encode/decode text\n",
    "        self.max_history = max_history  # Number of previous exchanges to consider\n",
    "        self.chat_history = []  # Stores the conversation history\n",
    "        self.device = device  # Ensure device is set\n",
    "        # Set pad_token to eos_token for models like GPT-2 that do not have a pad_token\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def reset_history(self):\n",
    "        self.chat_history = []\n",
    "\n",
    "    def get_response(self, user_input):\n",
    "        # Detect inappropriate or harmful content\n",
    "        if contains_disallowed_content(user_input):\n",
    "            return \"I'm sorry, but I cannot assist with that request.\"\n",
    "            \n",
    "        self.chat_history.append(user_input)\n",
    "        # Keep only the last max_history exchanges\n",
    "        history = self.chat_history[-self.max_history:]\n",
    "        # Combine the history into a single input string\n",
    "        input_text = \"<|endoftext|>\".join(history) + \"<|endoftext|>\"\n",
    "        \n",
    "        # Encode the input text with truncation and padding\n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors='pt',\n",
    "            max_length=512,  # Truncate if necessary\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        # Generate a response\n",
    "        output = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=50,  # Generate up to 50 new tokens\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            do_sample=True,  # Use sampling for more varied responses\n",
    "            top_p=0.92,      # Nucleus sampling parameter\n",
    "            top_k=40,        # Top-k sampling parameter\n",
    "            temperature=0.8, # Sampling temperature\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            num_return_sequences=1,  # Generate one response at a time\n",
    "            repetition_penalty=1.3,  # Penalize repetition for better response diversity\n",
    "            no_repeat_ngram_size=3   # Prevent repetition of n-grams\n",
    "        )\n",
    "        # Extract the generated tokens beyond the input length\n",
    "        generated_tokens = output[0][input_ids.size(-1):]\n",
    "        # Decode the generated tokens to text\n",
    "        reply = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        # Apply content filtering\n",
    "        if contains_disallowed_content(reply):\n",
    "            reply = \"I'm sorry, but I can't assist with that request.\"\n",
    "            \n",
    "        self.chat_history.append(reply)\n",
    "        return reply.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Chatbot class encapsulates the chatbot's functionality.\n",
    "- It maintains a conversation history to provide context for multi-turn conversations.\n",
    "- The get_response method generates a response using the model and applies content filtering.\n",
    "- Sampling parameters (top_p, top_k, temperature) control the diversity of the generated responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the Chatbot via Console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an instance of the Chatbot class.\n",
    "- A loop allows the user to interact with the chatbot via the console.\n",
    "- The conversation continues until the user types an exit command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hello! How can I assist you today?\n",
      "You: How was your day?\n",
      "Bot: Excellent. Very professional and easy going, too--the kind of guy you need on a rainy night to keep things running smoothly in the city center or backcountry...a man who never lets his guard down.  What did he do for us today\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the chatbot\n",
    "chatbot = Chatbot(model, tokenizer)\n",
    "\n",
    "print(\"Bot: Hello! How can I assist you today?\")\n",
    "\n",
    "# Manually specify user inputs in the notebook for each step\n",
    "user_input = \"How was your day?\"  # Example input\n",
    "print(f\"You: {user_input}\")\n",
    "response = chatbot.get_response(user_input)\n",
    "print(f\"Bot: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
